{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Follow instructions here to install https://github.com/openai/roboschool\n",
    "import roboschool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from lib.common import mkdir\n",
    "#from lib.model import ActorCritic\n",
    "import lib.model as models\n",
    "import lib.transforms as transforms\n",
    "from lib.multiprocessing_env import SubprocVecEnv\n",
    "from lib.environment import atari_env\n",
    "\n",
    "def setup(env_id):\n",
    "\n",
    "    # open read and overwrite.\n",
    "    with open(os.path.join('conf', env_id + '.json'), 'r+') as json_file:\n",
    "\n",
    "        data = json.load(json_file)\n",
    "\n",
    "        global ENV_ID\n",
    "        global NUM_INPUTS\n",
    "        global NUM_OUTPUTS\n",
    "        global NUM_ENVS\n",
    "        global HIDDEN_SIZE\n",
    "        global LEARNING_RATE\n",
    "        global GAMMA\n",
    "        global GAE_LAMBDA\n",
    "        global PPO_EPSILON\n",
    "        global CRITIC_DISCOUNT\n",
    "        global ENTROPY_BETA\n",
    "        global PPO_STEPS\n",
    "        global MINI_BATCH_SIZE\n",
    "        global PPO_EPOCHS\n",
    "        global TEST_EPOCHS\n",
    "        global NUM_TESTS\n",
    "        global TARGET_REWARD\n",
    "        global MODEL_NAME\n",
    "        global MODEL_CLASS\n",
    "        global TRANSFORM_NAME\n",
    "        global TRANSFORM_CLASS\n",
    "\n",
    "        ENV_ID          = data.setdefault('env_id', 'RoboschoolHalfCheetah-v1')\n",
    "        NUM_INPUTS      = data.setdefault('num_inputs', 26)\n",
    "        NUM_OUTPUTS     = data.setdefault('num_outputs', 6)\n",
    "        NUM_ENVS        = data.setdefault('num_envs', 1)\n",
    "        HIDDEN_SIZE     = data.get('hidden_size', 256)\n",
    "        LEARNING_RATE   = data.setdefault('learning_rate', 1e-4)\n",
    "        GAMMA           = data.setdefault('gamma', 0.99)\n",
    "        GAE_LAMBDA      = data.setdefault('gae_lambda', 0.95)\n",
    "        PPO_EPSILON     = data.setdefault('ppo_epsilon', 0.2)\n",
    "        CRITIC_DISCOUNT = data.setdefault('critic_discount', 0.5)\n",
    "        ENTROPY_BETA    = data.setdefault('entropy_beta', 0.001)\n",
    "        PPO_STEPS       = data.setdefault('ppo_steps', 256)\n",
    "        MINI_BATCH_SIZE = data.setdefault('mini_batch_size', 64)\n",
    "        PPO_EPOCHS      = data.setdefault('ppo_epochs', 10)\n",
    "        TEST_EPOCHS     = data.setdefault('test_epochs', 10)\n",
    "        NUM_TESTS       = data.setdefault('num_tests', 10)\n",
    "        TARGET_REWARD   = data.setdefault('target_reward', 2500)\n",
    "        MODEL_NAME      = data.setdefault('model_name', 'ActorCritic')\n",
    "        MODEL_CLASS     = getattr(models, MODEL_NAME)\n",
    "        TRANSFORM_NAME  = data.setdefault('transform_name', 'Identity')\n",
    "        TRANSFORM_CLASS = getattr(transforms, TRANSFORM_NAME)\n",
    "\n",
    "        # Transformations\n",
    "        if isinstance(NUM_INPUTS, list): NUM_INPUTS = tuple(NUM_INPUTS)\n",
    "\n",
    "        json_file.seek(0)  # go to beggining of file\n",
    "        json.dump(data, json_file)  # write content\n",
    "        json_file.truncate()  # clear any tail of old content\n",
    "\n",
    "\n",
    "def make_env(env_id):\n",
    "    # returns a function which creates a single environment\n",
    "    def _thunk():\n",
    "        #env = gym.make(env_id)\n",
    "        env = atari_env(env_id)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "def test_env(env, model, device, deterministic=True):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "\n",
    "        if isinstance(dist, torch.distributions.categorical.Categorical):\n",
    "            action = np.argmax(dist.probs.detach().cpu().numpy()) if deterministic \\\n",
    "                else int(dist.sample().cpu().numpy())\n",
    "\n",
    "        elif isinstance(dist, torch.distributions.normal.Normal):\n",
    "            action = dist.mean.detach().cpu().numpy()[0] if deterministic \\\n",
    "                else dist.sample().cpu().numpy()[0]\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, lam=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * \\\n",
    "            values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] * gae\n",
    "        # prepend to get correct order back\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "\n",
    "def ppo_iter(states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    # generates random mini-batches until we have covered the full batch\n",
    "    for _ in range(batch_size // MINI_BATCH_SIZE):\n",
    "        rand_ids = np.random.randint(0, batch_size, MINI_BATCH_SIZE)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "\n",
    "\n",
    "def ppo_update(frame_idx, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    count_steps = 0\n",
    "    sum_returns = 0.0\n",
    "    sum_advantage = 0.0\n",
    "    sum_loss_actor = 0.0\n",
    "    sum_loss_critic = 0.0\n",
    "    sum_entropy = 0.0\n",
    "    sum_loss_total = 0.0\n",
    "\n",
    "    # PPO EPOCHS is the number of times we will go through ALL the training data to make updates    \n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        # grabs random mini-batches several times until we have covered all data\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                                1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = CRITIC_DISCOUNT * critic_loss + actor_loss - ENTROPY_BETA * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # track statistics\n",
    "            sum_returns += return_.mean()\n",
    "            sum_advantage += advantage.mean()\n",
    "            sum_loss_actor += actor_loss\n",
    "            sum_loss_critic += critic_loss\n",
    "            sum_loss_total += loss\n",
    "            sum_entropy += entropy\n",
    "\n",
    "            count_steps += 1\n",
    "\n",
    "    writer.add_scalar(\"returns\", sum_returns / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"advantage\", sum_advantage / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_actor\", sum_loss_actor / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_critic\", sum_loss_critic / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"entropy\", sum_entropy / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_total\", sum_loss_total / count_steps, frame_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "ActorCriticLSTM(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTMCell(1024, 512)\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (actor_linear): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "setup('BreakoutDeterministic-v4')\n",
    "writer = SummaryWriter(comment=\"ppo_\" + ENV_ID)\n",
    "# Autodetect CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Device:', device)\n",
    "# Prepare environments\n",
    "envs = [make_env(ENV_ID) for i in range(NUM_ENVS)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = atari_env(ENV_ID)\n",
    "\n",
    "num_inputs = NUM_INPUTS #envs.observation_space\n",
    "num_outputs = NUM_OUTPUTS #envs.action_space\n",
    "model = MODEL_CLASS(num_inputs, num_outputs, hidden_size=HIDDEN_SIZE).to(device)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "frame_idx = 0\n",
    "train_epoch = 0\n",
    "best_reward = None\n",
    "state = envs.reset()\n",
    "early_stop = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 2, 0, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3866, -1.3866, -1.3866, -1.3856, -1.3869, -1.3866, -1.3856, -1.3856],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "values = []\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "masks = []\n",
    "\n",
    "for _ in range(PPO_STEPS):\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    dist, value = model(state)\n",
    "\n",
    "    action = dist.sample()\n",
    "    # each state, reward, done is a list of results from each parallel environment\n",
    "    next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    log_probs.append(log_prob)\n",
    "    values.append(value)\n",
    "    rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "    masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "\n",
    "    state = next_state\n",
    "    frame_idx += 1\n",
    "\n",
    "next_state = torch.FloatTensor(next_state).to(device)\n",
    "_, next_value = model(next_state)\n",
    "returns = compute_gae(next_value, rewards, masks,\n",
    "                      values, GAMMA, GAE_LAMBDA)\n",
    "\n",
    "returns = torch.cat(returns).detach()\n",
    "log_probs = torch.cat(log_probs).detach()\n",
    "values = torch.cat(values).detach()\n",
    "states = torch.cat(states)\n",
    "actions = torch.cat(actions)\n",
    "advantage = returns - values\n",
    "advantage = normalize(advantage)\n",
    "\n",
    "# For Categorical distribution\n",
    "if len(log_probs.size()) == 1:\n",
    "    log_probs = log_probs.unsqueeze(dim=1)\n",
    "\n",
    "if len(actions.size()) == 1:\n",
    "    actions = actions.unsqueeze(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, action, old_log_probs, return_, advantage in ppo_iter(states, actions, log_probs, returns, advantage):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, value = model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = dist.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_probs = dist.log_prob(action.squeeze()).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = (new_log_probs - old_log_probs).exp()\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5298],\n",
       "        [-0.5028],\n",
       "        [-0.5130],\n",
       "        [-0.4615]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surr1 = ratio * advantage\n",
    "surr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5298],\n",
       "        [-0.5028],\n",
       "        [-0.5130],\n",
       "        [-0.4615]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_param=0.2\n",
    "surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "surr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5017, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(surr1, surr2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(return_ - value).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0362],\n",
       "        [0.0327],\n",
       "        [0.0328],\n",
       "        [0.0321]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SchoolOfAI",
   "language": "python",
   "name": "schoolofai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
