{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Follow instructions here to install https://github.com/openai/roboschool\n",
    "#import roboschool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from lib.common import mkdir\n",
    "#from lib.model import ActorCritic\n",
    "import lib.model as models\n",
    "from lib.multiprocessing_env import SubprocVecEnv\n",
    "from lib.environment import atari_env\n",
    "from lib.config import setup\n",
    "\n",
    "def make_env(env_id):\n",
    "    # returns a function which creates a single environment\n",
    "    def _thunk():\n",
    "        #env = gym.make(env_id)\n",
    "        env = atari_env(env_id)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "def test_env(env, model, device, deterministic=True):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "\n",
    "        if isinstance(dist, torch.distributions.categorical.Categorical):\n",
    "            action = np.argmax(dist.probs.detach().cpu().numpy()) if deterministic \\\n",
    "                else int(dist.sample().cpu().numpy())\n",
    "\n",
    "        elif isinstance(dist, torch.distributions.normal.Normal):\n",
    "            action = dist.mean.detach().cpu().numpy()[0] if deterministic \\\n",
    "                else dist.sample().cpu().numpy()[0]\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x\n",
    "\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, lam=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * \\\n",
    "            values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] * gae\n",
    "        # prepend to get correct order back\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "def ppo_iter(states, actions, log_probs, returns, advantage, mini_batch_size):\n",
    "    batch_size = states.shape[0]\n",
    "    # generates random mini-batches until we have covered the full batch\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "\n",
    "def ppo_update(frame_idx, states, actions, log_probs, returns, advantages, clip_param, \n",
    "    epochs, mini_batch_size, critic_discount, entropy_beta, device):\n",
    "\n",
    "    count_steps = 0\n",
    "    sum_returns = 0.0\n",
    "    sum_advantage = 0.0\n",
    "    sum_loss_actor = 0.0\n",
    "    sum_loss_critic = 0.0\n",
    "    sum_entropy = 0.0\n",
    "    sum_loss_total = 0.0\n",
    "\n",
    "    # PPO EPOCHS is the number of times we will go through ALL the training data to make updates    \n",
    "    for _ in range(epochs):\n",
    "        # grabs random mini-batches several times until we have covered all data\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(states, actions, log_probs, returns, advantages, mini_batch_size):\n",
    "            \n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action).to(device)\n",
    "            old_log_probs = torch.FloatTensor(old_log_probs).to(device)\n",
    "            advantage = torch.FloatTensor(advantage).to(device)\n",
    "            return_ = torch.FloatTensor(return_).to(device)\n",
    "\n",
    "            dist, value = model(state)\n",
    "            \n",
    "            entropy = dist.entropy().mean()\n",
    "            #new_log_probs = dist.log_prob(action)\n",
    "            new_log_probs = dist.log_prob(action.squeeze()).unsqueeze(dim=1)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
    "                                1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = critic_discount * critic_loss + actor_loss - entropy_beta * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # track statistics\n",
    "            sum_returns += return_.mean()\n",
    "            sum_advantage += advantage.mean()\n",
    "            sum_loss_actor += actor_loss\n",
    "            sum_loss_critic += critic_loss\n",
    "            sum_loss_total += loss\n",
    "            sum_entropy += entropy\n",
    "\n",
    "            count_steps += 1\n",
    "\n",
    "    writer.add_scalar(\"returns\", sum_returns / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"advantage\", sum_advantage / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_actor\", sum_loss_actor / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_critic\", sum_loss_critic / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"entropy\", sum_entropy / count_steps, frame_idx)\n",
    "    writer.add_scalar(\"loss_total\", sum_loss_total / count_steps, frame_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "ActorCriticLSTM(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxp4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTMCell(1024, 512)\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (actor_linear): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    conf = setup('BreakoutNoFrameskip-v4')\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print('Device:', device)\n",
    "\n",
    "    # Prepare environments\n",
    "    envs = [make_env(conf.ENV_ID) for i in range(conf.NUM_ENVS)]\n",
    "    envs = SubprocVecEnv(envs)\n",
    "    \n",
    "    env = atari_env(conf.ENV_ID)\n",
    "    \n",
    "    num_inputs = conf.NUM_INPUTS #envs.observation_space\n",
    "    num_outputs = conf.NUM_OUTPUTS #envs.action_space\n",
    "\n",
    "    model = conf.MODEL_CLASS(num_inputs, num_outputs, hidden_size=conf.HIDDEN_SIZE).to(device)\n",
    "    print(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=conf.LEARNING_RATE)\n",
    "\n",
    "    frame_idx = 0\n",
    "    train_epoch = 0\n",
    "    best_reward = None\n",
    "\n",
    "    state = envs.reset()\n",
    "    early_stop = False\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(conf.PPO_STEPS):\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = model(state)\n",
    "            \n",
    "            action = dist.sample()\n",
    "            \n",
    "            # each state, reward, done is a list of results from each parallel environment\n",
    "            next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "            \n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            # Save Episodes\n",
    "            log_probs.append(log_prob.cpu().detach().numpy())\n",
    "            values.append(value.cpu().detach().numpy())\n",
    "            rewards.append(np.expand_dims(reward, 1))\n",
    "            masks.append(np.expand_dims(1-done, 1))\n",
    "            states.append(state.cpu().detach().numpy())\n",
    "            actions.append(action.cpu().detach().numpy())\n",
    "\n",
    "            state = next_state\n",
    "            frame_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf022e1e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(states[138][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = returns - values\n",
    "advantages = normalize(advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(log_probs.shape) == 1:\n",
    "            log_probs = np.expand_dims(log_probs, 1)\n",
    "\n",
    "if len(actions.shape) == 1:\n",
    "            actions = np.expand_dims(actions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, action, old_log_probs, return_, advantage in ppo_iter(states, actions, log_probs, returns, advantages, 128):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    count_steps = 0\n",
    "    sum_returns = 0.0\n",
    "    sum_advantage = 0.0\n",
    "    sum_loss_actor = 0.0\n",
    "    sum_loss_critic = 0.0\n",
    "    sum_entropy = 0.0\n",
    "    sum_loss_total = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.FloatTensor(state).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, value = model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = dist.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [3.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [2.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = torch.FloatTensor(action).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_probs = dist.log_prob(action.squeeze()).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_log_probs = torch.FloatTensor(old_log_probs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (new_log_probs - old_log_probs).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage = torch.FloatTensor(advantage).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr1 = ratio * advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr2 = torch.clamp(ratio, 1.0 - 0.2, 1.0 + 0.2) * advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_loss = - torch.min(surr1, surr2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_ = torch.FloatTensor(return_).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss = (return_ - value).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0139, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_returns += return_.mean()\n",
    "sum_advantage += advantage.mean()\n",
    "sum_loss_actor += actor_loss\n",
    "sum_loss_critic += critic_loss\n",
    "sum_loss_total += loss\n",
    "sum_entropy += entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0534, device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-d630b25a3b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ppo_update(frame_idx, states, actions, log_probs, returns, advantage,\n\u001b[0m\u001b[1;32m      2\u001b[0m             conf.PPO_EPSILON, conf.PPO_EPOCHS, conf.MINI_BATCH_SIZE, conf.CRITIC_DISCOUNT, conf.ENTROPY_BETA, device)\n",
      "\u001b[0;32m<ipython-input-2-7a474bfa1be7>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(frame_idx, states, actions, log_probs, returns, advantages, clip_param, epochs, mini_batch_size, critic_discount, entropy_beta, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# grabs random mini-batches several times until we have covered all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mppo_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7a474bfa1be7>\u001b[0m in \u001b[0;36mppo_iter\u001b[0;34m(states, actions, log_probs, returns, advantage, mini_batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mppo_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# generates random mini-batches until we have covered the full batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "ppo_update(frame_idx, states, actions, log_probs, returns, advantage,\n",
    "            conf.PPO_EPSILON, conf.PPO_EPOCHS, conf.MINI_BATCH_SIZE, conf.CRITIC_DISCOUNT, conf.ENTROPY_BETA, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
